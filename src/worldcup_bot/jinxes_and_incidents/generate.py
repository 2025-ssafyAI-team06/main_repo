# generate.py
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_upstage import ChatUpstage
from langchain_core.runnables import Runnable

from worldcup_bot.jinxes_and_incidents.search import retrieve

import os


def build_jinxes_rag_chain() -> Runnable:
    template = (
        "ë‹¹ì‹ ì€ ì œê³µëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë„ì›€ì´ ë˜ëŠ” í•œêµ­ì–´ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "í•­ìƒ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë§Œ í™œìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•´ì¤˜ì•¼ í•©ë‹ˆë‹¤.\n\n"
        "ì»¨í…ìŠ¤íŠ¸:\n{context}\n\n"
        "ì§ˆë¬¸: {question}\n"
        "ë‹µë³€:"
    )

    prompt = ChatPromptTemplate.from_template(template)
    llm = ChatUpstage(model="solar-pro", temperature=0)
    rag_chain = prompt | llm | StrOutputParser()
    return rag_chain


def generate(state: dict, rag_chain) -> dict:
    print("---GENERATE ANSWER---")
    question = state["question"]
    docs = state["documents"]
    answer = rag_chain.invoke({"context": docs, "question": question})
    return {"question": question, "documents": docs, "generation": answer}


def run_jinxes_and_incidents_pipeline(user_query: str, retriever, rag_chain) -> str:
    state = {"question": user_query}
    state = retrieve(state, retriever)
    state = generate(state, rag_chain)

    print("\nğŸ—£ï¸ ìµœì¢… ì‘ë‹µ:\n")
    print(state["generation"])
    return state["generation"]
